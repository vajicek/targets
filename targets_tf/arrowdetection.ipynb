{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arrow detection using tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create folder hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "files = pickle.load(open('files.pickle', 'rb'))\n",
    "paths = pickle.load(open('paths.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5KJTnkfpfDC"
   },
   "source": [
    "## 2. Create Label Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p1BVDWo7pfDC"
   },
   "outputs": [],
   "source": [
    "labels = [{'name':'1', 'id':1}]\n",
    "\n",
    "with open(files['LABELMAP'], 'w') as f:\n",
    "    for label in labels:\n",
    "        f.write('item { \\n')\n",
    "        f.write('\\tname:\\'{}\\'\\n'.format(label['name']))\n",
    "        f.write('\\tid:{}\\n'.format(label['id']))\n",
    "        f.write('}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C88zyVELpfDC"
   },
   "source": [
    "## 3. Prepere test and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "#DATA='/home/ox330/data/targets/all/'\n",
    "#DATA='/home/ox330/data/targets/squares/'\n",
    "DATA='/home/ox330/data/targets/points/'\n",
    "\n",
    "annotation_jsons = glob.glob(DATA + '*.json')\n",
    "test_size = int(len(annotation_jsons) / 3)\n",
    "#test_size = int(len(annotation_jsons) - 10)\n",
    "test = annotation_jsons[:test_size]\n",
    "train = annotation_jsons[test_size:]\n",
    "\n",
    "!mkdir -p {os.path.join(paths['IMAGE_PATH'], 'test')}\n",
    "!mkdir -p {os.path.join(paths['IMAGE_PATH'], 'train')}\n",
    "\n",
    "!rm {os.path.join(paths['IMAGE_PATH'], 'test')}/*\n",
    "!rm {os.path.join(paths['IMAGE_PATH'], 'train')}/*\n",
    "\n",
    "for test1 in test:\n",
    "    !cp {test1} {os.path.join(paths['IMAGE_PATH'], 'test')}\n",
    "    !cp {os.path.splitext(test1)[0]+'.jpg'} {os.path.join(paths['IMAGE_PATH'], 'test')}\n",
    "for train1 in train:\n",
    "    !cp {train1} {os.path.join(paths['IMAGE_PATH'], 'train')}\n",
    "    !cp {os.path.splitext(train1)[0]+'.jpg'} {os.path.join(paths['IMAGE_PATH'], 'train')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C88zyVELpfDC"
   },
   "source": [
    "## 4. tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import generate_tfrecord as gen;\n",
    "gen.json_to_tfrecord(os.path.join(paths['IMAGE_PATH'], 'train'), files['LABELMAP'],\n",
    "    os.path.join(paths['IMAGE_PATH'], 'train'),\n",
    "    os.path.join(paths['ANNOTATION_PATH'], 'train.record'), 40)\n",
    "gen.json_to_tfrecord(os.path.join(paths['IMAGE_PATH'], 'test'), files['LABELMAP'],\n",
    "    os.path.join(paths['IMAGE_PATH'], 'test'),\n",
    "    os.path.join(paths['ANNOTATION_PATH'], 'test.record'), 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qT4QU7pLpfDE"
   },
   "source": [
    "## 5. Copy Model Config to Training Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cOjuTFbwpfDF"
   },
   "outputs": [],
   "source": [
    "!cp {files['PRETRAINED_MODEL_CONFIG']} {paths['CHECKPOINT_PATH']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ga8gpNslpfDF"
   },
   "source": [
    "## 6. Update Config For Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z9hRrO_ppfDF"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.protos import pipeline_pb2\n",
    "from google.protobuf import text_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "id": "c2A0mn4ipfDF"
   },
   "outputs": [],
   "source": [
    "# config from pretrained model\n",
    "config = config_util.get_configs_from_pipeline_file(files['PIPELINE_CONFIG'])\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "with tf.io.gfile.GFile(files['PIPELINE_CONFIG'], \"r\") as f:\n",
    "    text_format.Merge(f.read(), pipeline_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "id": "rP43Ph0JpfDG"
   },
   "outputs": [],
   "source": [
    "# update model training parameters\n",
    "pipeline_config.model.ssd.num_classes = len(labels)\n",
    "# pipeline_config.model.ssd.image_resizer.fixed_shape_resizer.height = 512\n",
    "# pipeline_config.model.ssd.image_resizer.fixed_shape_resizer.width = 512\n",
    "\n",
    "pipeline_config.train_config.batch_size = 2\n",
    "pipeline_config.train_config.fine_tune_checkpoint = files['PRETRAINED_MODEL_CKP0']\n",
    "pipeline_config.train_config.fine_tune_checkpoint_type = \"detection\"\n",
    "\n",
    "pipeline_config.train_input_reader.label_map_path = files['LABELMAP']\n",
    "pipeline_config.train_input_reader.tf_record_input_reader.input_path[:] = [\n",
    "    os.path.join(paths['ANNOTATION_PATH'], 'train.record')]\n",
    "\n",
    "pipeline_config.eval_input_reader[0].label_map_path = files['LABELMAP']\n",
    "pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[:] = [\n",
    "    os.path.join(paths['ANNOTATION_PATH'], 'test.record')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "id": "oJvfgwWqpfDG"
   },
   "outputs": [],
   "source": [
    "#store updated config\n",
    "config_text = text_format.MessageToString(pipeline_config)\n",
    "with tf.io.gfile.GFile(files['PIPELINE_CONFIG'], \"wb\") as f:\n",
    "    f.write(config_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-Y2UQmQpfDG"
   },
   "outputs": [],
   "source": [
    "TRAINING_SCRIPT = os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection', 'model_main_tf2.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jMP2XDfQpfDH"
   },
   "outputs": [],
   "source": [
    "command = \"python3 {} --model_dir={} --pipeline_config_path={} --num_train_steps=2000\".format(\n",
    "    TRAINING_SCRIPT,\n",
    "    paths['CHECKPOINT_PATH'],\n",
    "    files['PIPELINE_CONFIG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm {paths['CHECKPOINT_PATH']}/ckpt-*\n",
    "!rm {paths['CHECKPOINT_PATH']}/checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A4OXXi-ApfDH",
    "outputId": "117a0e83-012b-466e-b7a6-ccaa349ac5ab"
   },
   "outputs": [],
   "source": [
    "print(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = \"python3 {} --model_dir={} --pipeline_config_path={} --checkpoint_dir={}\".format(\n",
    "    TRAINING_SCRIPT,\n",
    "    paths['CHECKPOINT_PATH'],\n",
    "    files['PIPELINE_CONFIG'],\n",
    "    paths['CHECKPOINT_PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{command}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
