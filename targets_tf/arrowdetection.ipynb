{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arrow detection using tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create folder hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM_MODEL_NAME = 'my_ssd_mobnet' \n",
    "# PRETRAINED_MODEL_NAME = 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8'\n",
    "# PRETRAINED_MODEL_URL = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz'\n",
    "\n",
    "# CUSTOM_MODEL_NAME = 'my_ssd' \n",
    "# PRETRAINED_MODEL_NAME = 'ssd_resnet152_v1_fpn_1024x1024_coco17_tpu-8'\n",
    "# PRETRAINED_MODEL_URL = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet152_v1_fpn_1024x1024_coco17_tpu-8.tar.gz'\n",
    "\n",
    "# CUSTOM_MODEL_NAME = 'my_ssd_resnet152' \n",
    "# PRETRAINED_MODEL_NAME = 'ssd_resnet152_v1_fpn_640x640_coco17_tpu-8'\n",
    "# PRETRAINED_MODEL_URL = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet152_v1_fpn_640x640_coco17_tpu-8.tar.gz'\n",
    "\n",
    "CUSTOM_MODEL_NAME = 'my_ssd_resnet50' \n",
    "PRETRAINED_MODEL_NAME = 'ssd_resnet50_v1_fpn_1024x1024_coco17_tpu-8'\n",
    "PRETRAINED_MODEL_URL = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_1024x1024_coco17_tpu-8.tar.gz'\n",
    "\n",
    "# CUSTOM_MODEL_NAME = 'my_centernet_mobilenetv2fpn' \n",
    "# PRETRAINED_MODEL_NAME = 'centernet_mobilenetv2_fpn_kpts'\n",
    "# PRETRAINED_MODEL_URL = 'https://tfhub.dev/tensorflow/centernet/hourglass_1024x1024_kpts/1?tf-hub-format=compressed'\n",
    "TF_RECORD_SCRIPT_NAME = 'generate_tfrecord.py'\n",
    "LABEL_MAP_NAME = 'label_map.pbtxt'\n",
    "\n",
    "paths = {\n",
    "    'WORKSPACE_PATH': os.path.join('Tensorflow', 'workspace'),\n",
    "    'SCRIPTS_PATH': os.path.join('Tensorflow','scripts'),\n",
    "    'APIMODEL_PATH': os.path.join('Tensorflow','models'),\n",
    "    'ANNOTATION_PATH': os.path.join('Tensorflow', 'workspace','annotations'),\n",
    "    'IMAGE_PATH': os.path.join('Tensorflow', 'workspace','images'),\n",
    "    'MODEL_PATH': os.path.join('Tensorflow', 'workspace','models'),\n",
    "    'PRETRAINED_MODEL_PATH': os.path.join('Tensorflow', 'workspace','pre-trained-models'),\n",
    "\n",
    "    'CHECKPOINT_PATH': os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME), \n",
    "    'OUTPUT_PATH': os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME, 'export'), \n",
    "    'TFJS_PATH':os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME, 'tfjsexport'), \n",
    "    'TFLITE_PATH':os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME, 'tfliteexport'), \n",
    "    'PROTOC_PATH':os.path.join('Tensorflow','protoc')\n",
    "}\n",
    "\n",
    "files = {\n",
    "    'PIPELINE_CONFIG':os.path.join('Tensorflow', 'workspace','models', CUSTOM_MODEL_NAME, 'pipeline.config'),\n",
    "    'TF_RECORD_SCRIPT': TF_RECORD_SCRIPT_NAME, \n",
    "    'LABELMAP': os.path.join(paths['ANNOTATION_PATH'], LABEL_MAP_NAME)\n",
    "}\n",
    "\n",
    "# create workspace directories\n",
    "for path in paths.values():\n",
    "    if not os.path.exists(path):\n",
    "        if os.name == 'posix':\n",
    "            !mkdir -p {path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget --no-proxy {PRETRAINED_MODEL_URL} -O {PRETRAINED_MODEL_NAME}.tar.gz\n",
    "!mv {PRETRAINED_MODEL_NAME + '.tar.gz'} {paths['PRETRAINED_MODEL_PATH']}\n",
    "!cd {paths['PRETRAINED_MODEL_PATH']} && tar -zxvf {PRETRAINED_MODEL_NAME + '.tar.gz'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clone tensorflow and build models API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/tensorflow/models {paths['APIMODEL_PATH']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build API buffers\n",
    "!cd Tensorflow/models/research && protoc object_detection/protos/*.proto --python_out=.\n",
    "# Install TensorFlow Object Detection\n",
    "!cd Tensorflow/models/research && cp object_detection/packages/tf2/setup.py . && python3 -m pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERIFICATION_SCRIPT = os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection', 'builders', 'model_builder_tf2_test.py')\n",
    "# Verify Installation\n",
    "!python3 {VERIFICATION_SCRIPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import object_detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5KJTnkfpfDC"
   },
   "source": [
    "## 4. Create Label Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p1BVDWo7pfDC"
   },
   "outputs": [],
   "source": [
    "labels = [{'name':'1', 'id':1}]\n",
    "\n",
    "# labels = [{'name':'10', 'id':1},\n",
    "#           {'name':'9', 'id':2},\n",
    "#           {'name':'8', 'id':3},\n",
    "#           {'name':'7', 'id':4},\n",
    "#           {'name':'6', 'id':5},\n",
    "#           {'name':'5', 'id':6},\n",
    "#           {'name':'4', 'id':7},\n",
    "#           {'name':'3', 'id':8},\n",
    "#           {'name':'2', 'id':9},\n",
    "#           {'name':'1', 'id':10},\n",
    "#           {'name':'M', 'id':11},\n",
    "#           {'name':'X', 'id':12}]\n",
    "\n",
    "with open(files['LABELMAP'], 'w') as f:\n",
    "    for label in labels:\n",
    "        f.write('item { \\n')\n",
    "        f.write('\\tname:\\'{}\\'\\n'.format(label['name']))\n",
    "        f.write('\\tid:{}\\n'.format(label['id']))\n",
    "        f.write('}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "#DATA='/home/ox330/data/targets/all/'\n",
    "#DATA='/home/ox330/data/targets/squares/'\n",
    "DATA='/home/ox330/data/targets/points/'\n",
    "\n",
    "annotation_jsons = glob.glob(DATA + '*.json')\n",
    "test_size = int(len(annotation_jsons) / 3)\n",
    "#test_size = int(len(annotation_jsons) - 10)\n",
    "test = annotation_jsons[:test_size]\n",
    "train = annotation_jsons[test_size:]\n",
    "\n",
    "!mkdir -p {os.path.join(paths['IMAGE_PATH'], 'test')}\n",
    "!mkdir -p {os.path.join(paths['IMAGE_PATH'], 'train')}\n",
    "\n",
    "!rm {os.path.join(paths['IMAGE_PATH'], 'test')}/*\n",
    "!rm {os.path.join(paths['IMAGE_PATH'], 'train')}/*\n",
    "\n",
    "for test1 in test:\n",
    "    !cp {test1} {os.path.join(paths['IMAGE_PATH'], 'test')}\n",
    "    !cp {os.path.splitext(test1)[0]+'.jpg'} {os.path.join(paths['IMAGE_PATH'], 'test')}\n",
    "for train1 in train:\n",
    "    !cp {train1} {os.path.join(paths['IMAGE_PATH'], 'train')}\n",
    "    !cp {os.path.splitext(train1)[0]+'.jpg'} {os.path.join(paths['IMAGE_PATH'], 'train')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C88zyVELpfDC"
   },
   "source": [
    "## 5. Create TF records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import generate_tfrecord as gen;\n",
    "gen.json_to_tfrecord(os.path.join(paths['IMAGE_PATH'], 'train'), files['LABELMAP'],\n",
    "    os.path.join(paths['IMAGE_PATH'], 'train'),\n",
    "    os.path.join(paths['ANNOTATION_PATH'], 'train.record'), 40)\n",
    "gen.json_to_tfrecord(os.path.join(paths['IMAGE_PATH'], 'test'), files['LABELMAP'],\n",
    "    os.path.join(paths['IMAGE_PATH'], 'test'),\n",
    "    os.path.join(paths['ANNOTATION_PATH'], 'test.record'), 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qT4QU7pLpfDE"
   },
   "source": [
    "## 6. Copy Model Config to Training Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cOjuTFbwpfDF"
   },
   "outputs": [],
   "source": [
    "!cp {os.path.join(paths['PRETRAINED_MODEL_PATH'], PRETRAINED_MODEL_NAME, 'pipeline.config')} \\\n",
    "    {os.path.join(paths['CHECKPOINT_PATH'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ga8gpNslpfDF"
   },
   "source": [
    "## 7. Update Config For Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z9hRrO_ppfDF"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.protos import pipeline_pb2\n",
    "from google.protobuf import text_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2A0mn4ipfDF"
   },
   "outputs": [],
   "source": [
    "# config from pretrained model\n",
    "config = config_util.get_configs_from_pipeline_file(files['PIPELINE_CONFIG'])\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "with tf.io.gfile.GFile(files['PIPELINE_CONFIG'], \"r\") as f:\n",
    "    text_format.Merge(f.read(), pipeline_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rP43Ph0JpfDG"
   },
   "outputs": [],
   "source": [
    "# update model training parameters\n",
    "pipeline_config.model.ssd.num_classes = len(labels)\n",
    "pipeline_config.model.ssd.image_resizer.fixed_shape_resizer.height = 512\n",
    "pipeline_config.model.ssd.image_resizer.fixed_shape_resizer.width = 512\n",
    "\n",
    "pipeline_config.train_config.batch_size = 1\n",
    "pipeline_config.train_config.fine_tune_checkpoint = os.path.join(paths['PRETRAINED_MODEL_PATH'],\n",
    "                                                                 PRETRAINED_MODEL_NAME, 'checkpoint', 'ckpt-0')\n",
    "pipeline_config.train_config.fine_tune_checkpoint_type = \"detection\"\n",
    "\n",
    "pipeline_config.train_input_reader.label_map_path = files['LABELMAP']\n",
    "pipeline_config.train_input_reader.tf_record_input_reader.input_path[:] = [os.path.join(paths['ANNOTATION_PATH'],\n",
    "                                                                                        'train.record')]\n",
    "pipeline_config.eval_input_reader[0].label_map_path = files['LABELMAP']\n",
    "pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[:] = [os.path.join(paths['ANNOTATION_PATH'],\n",
    "                                                                                          'test.record')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oJvfgwWqpfDG"
   },
   "outputs": [],
   "source": [
    "#store updated config\n",
    "config_text = text_format.MessageToString(pipeline_config)\n",
    "with tf.io.gfile.GFile(files['PIPELINE_CONFIG'], \"wb\") as f:\n",
    "    f.write(config_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-Y2UQmQpfDG"
   },
   "outputs": [],
   "source": [
    "TRAINING_SCRIPT = os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection', 'model_main_tf2.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jMP2XDfQpfDH"
   },
   "outputs": [],
   "source": [
    "command = \"python3 {} --model_dir={} --pipeline_config_path={} --num_train_steps=2000\".format(\n",
    "    TRAINING_SCRIPT,\n",
    "    paths['CHECKPOINT_PATH'],\n",
    "    files['PIPELINE_CONFIG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm Tensorflow/workspace/models/{CUSTOM_MODEL_NAME}/ckpt-*\n",
    "!rm Tensorflow/workspace/models/{CUSTOM_MODEL_NAME}/checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A4OXXi-ApfDH",
    "outputId": "117a0e83-012b-466e-b7a6-ccaa349ac5ab"
   },
   "outputs": [],
   "source": [
    "print(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = \"python3 {} --model_dir={} --pipeline_config_path={} --checkpoint_dir={}\".format(\n",
    "    TRAINING_SCRIPT,\n",
    "    paths['CHECKPOINT_PATH'],\n",
    "    files['PIPELINE_CONFIG'],\n",
    "    paths['CHECKPOINT_PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{command}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
